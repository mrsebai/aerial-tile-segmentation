{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prototype of Segmenter based on tf.data API\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "import tempfile\n",
    "import platform\n",
    "import imageio\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import LogLocator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# tensorflow_io can handle TIFF images (not ready for TF 2.0)\n",
    "# import tensorflow_io.image as image_io\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "# eager execution is default for TF 2.0\n",
    "#tf.enable_eager_execution()\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "pd.options.display.float_format = '{:.4f}'.format\n",
    "print(\"Python version     : \" + platform.python_version())\n",
    "print(\"Tensorflow version : \" + tf.version.VERSION)\n",
    "print(\"Keras version      : \" + tf.keras.__version__)\n",
    "print(\"Numpy version      : \" + np.__version__)\n",
    "print(\"Pandas version     : \" + pd.__version__)\n",
    "print(\"Imageio version    : \" + imageio.__version__)\n",
    "print(\"GPU available      : \" + str(tf.test.is_gpu_available()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ISPRS Project Constants\n",
    "---\n",
    "If one would like to train using the ISPRS dataset, the following cells should be run exclusively (INRIA Building cells not to be run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining tiff image and mask location\n",
    "train_img_dir  = '../train_dir/potsdam_aerials/'\n",
    "train_mask_dir = '../train_dir/potsdam_masks/'\n",
    "val_img_dir    = '../validation_dir/potsdam_aerials/'\n",
    "val_mask_dir   = '../validation_dir/potsdam_masks/'\n",
    "# let's build two list containing the path of the aerials and mask files\n",
    "train_src = Path(train_img_dir)\n",
    "train_ref = Path(train_mask_dir)\n",
    "val_src   = Path(val_img_dir)\n",
    "val_ref   = Path(val_mask_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images     = sorted([str(x) for x in train_src.iterdir() if x.is_file() and x.suffix == '.tif'])\n",
    "train_masks      = sorted([str(x) for x in train_ref.iterdir() if x.is_file() and x.suffix == '.tif'])\n",
    "val_images       = sorted([str(x) for x in val_src.iterdir()   if x.is_file() and x.suffix == '.tif'])\n",
    "val_masks        = sorted([str(x) for x in val_ref.iterdir()   if x.is_file() and x.suffix == '.tif'])\n",
    "\n",
    "# checking the equality of the two list lenghts\n",
    "assert len(train_images) == len(train_masks), \" (Error) The train Aerial image count does not match Mask counts!\"\n",
    "assert len(val_images)   == len(val_masks), \" (Error) The Validation Aerial image count does not match Mask counts!\"\n",
    "len_train_images = len(train_images)\n",
    "len_val_images   = len(val_images)\n",
    "len_train_images, len_val_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ordered lists of colors and it's corresponding class name in [R, G, B] format\n",
    "color_list  = [ [1, 0, 0], [0, 0, 1], [1, 1, 1], [0, 1, 1], [0, 1, 0], [1, 1, 0]]\n",
    "color_names = ['Background', 'Building', 'Roads', 'Vegetation', 'Tree', 'Car']\n",
    "# Patch extraction parameters\n",
    "# it works jointly on the aerial and the mask image to keep them aligned\n",
    "TILE_SIZE   = 6000 \n",
    "PATCH_SIZE  = 256\n",
    "PATCH_STRIDE= 256\n",
    "PATCH_RATE  = 1\n",
    "SIZES       = [1, PATCH_SIZE, PATCH_SIZE, 1] \n",
    "STRIDES     = [1, PATCH_STRIDE, PATCH_STRIDE, 1] \n",
    "RATES       = [1, PATCH_RATE, PATCH_RATE, 1] \n",
    "PADDING     = 'VALID'\n",
    "\n",
    "def compute_tile_patch_number(rates=[1,2,3]):\n",
    "    \"\"\" we expect that all tile share the same size \"\"\"\n",
    "    with tf.device('/CPU:0'):\n",
    "        aerial = tf.constant(imageio.imread(train_images[0]))\n",
    "        total  = 0\n",
    "        for r in rates:\n",
    "            patches = tf.image.extract_patches(images=tf.expand_dims(aerial, axis=0), sizes=SIZES, strides=STRIDES, rates=[1,r,r,1], padding=PADDING)            \n",
    "            total += patches.shape[1] * patches.shape[2]\n",
    "        return total\n",
    "    \n",
    "# old formula to compute patch number\n",
    "# PATCH_NUMBER= ((TILE_SIZE - PATCH_SIZE)//PATCH_STRIDE + 1)**2\n",
    "\n",
    "# choose this line if you want a dataset with 3 different scales\n",
    "# TRAIN_PATCH_NUMBER = compute_tile_patch_number(rates=[1,2,3])\n",
    "# chosse this line if you want only one scale\n",
    "TRAIN_PATCH_NUMBER = compute_tile_patch_number(rates=[1])\n",
    "VALID_PATCH_NUMBER = compute_tile_patch_number(rates=[1])\n",
    "TRAIN_PATCH_NUMBER, VALID_PATCH_NUMBER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SHAPE  = (256, 256, 3)\n",
    "PATCH_RESIZE = (128, 128)\n",
    "NUM_CLASSES  = 6\n",
    "BATCH_SIZE   = 50\n",
    "EPOCHS       = 50\n",
    "NUM_TRAIN_EXAMPLES = TRAIN_PATCH_NUMBER * len_train_images \n",
    "NUM_VAL_EXAMPLES   = VALID_PATCH_NUMBER * len_val_images \n",
    "LEARNING_RATE   = 0.0001\n",
    "TRAIN_STEPS_PER_EPOCH = int(np.ceil(NUM_TRAIN_EXAMPLES / float(BATCH_SIZE)))\n",
    "VAL_STEPS_PER_EPOCH   = int(np.ceil(NUM_VAL_EXAMPLES / float(BATCH_SIZE)))\n",
    "TRAIN_STEPS_PER_EPOCH, VAL_STEPS_PER_EPOCH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INRIA Project Constants\n",
    "---\n",
    "(Remainder) chose between ISPRS or INRIA constant to be run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining tiff image and mask location\n",
    "inria_train_img_dir  = '/datasets/InriaAerial/AerialImageDataset/train/images/'\n",
    "inria_train_mask_dir = '/datasets/InriaAerial/AerialImageDataset/train/gt/'\n",
    "inria_val_img_dir    = '/datasets/InriaAerial/AerialImageDataset/valid/images/'\n",
    "inria_val_mask_dir   = '/datasets/InriaAerial/AerialImageDataset/valid/gt/'\n",
    "# let's build two list containing the path of the aerials and mask files\n",
    "train_src = Path(inria_train_img_dir)\n",
    "train_ref = Path(inria_train_mask_dir)\n",
    "val_src   = Path(inria_val_img_dir)\n",
    "val_ref   = Path(inria_val_mask_dir)\n",
    "train_images     = sorted([str(x) for x in train_src.iterdir() if x.is_file() and x.suffix == '.tif'])\n",
    "train_masks      = sorted([str(x) for x in train_ref.iterdir() if x.is_file() and x.suffix == '.tif'])\n",
    "val_images       = sorted([str(x) for x in val_src.iterdir()   if x.is_file() and x.suffix == '.tif'])\n",
    "val_masks        = sorted([str(x) for x in val_ref.iterdir()   if x.is_file() and x.suffix == '.tif'])\n",
    "\n",
    "# checking the equality of the two list lenghts\n",
    "assert len(train_images) == len(train_masks), \" (Error) The train Aerial image count does not match Mask counts!\"\n",
    "assert len(val_images)   == len(val_masks), \" (Error) The Validation Aerial image count does not match Mask counts!\"\n",
    "len_train_images = len(train_images)\n",
    "len_val_images   = len(val_images)\n",
    "len_train_images, len_val_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ordered lists of colors and it's corresponding class name in [R, G, B] format\n",
    "color_list  = [ [0, 0, 0], [1, 1, 1]]\n",
    "color_names = ['Background', 'Building']\n",
    "# patch extraction parameters\n",
    "TILE_SIZE   = 5000 \n",
    "PATCH_SIZE  = 256\n",
    "PATCH_STRIDE= 256\n",
    "PATCH_RATE  = 1\n",
    "SIZES       = [1, PATCH_SIZE, PATCH_SIZE, 1] \n",
    "STRIDES     = [1, PATCH_STRIDE, PATCH_STRIDE, 1] \n",
    "RATES       = [1, PATCH_RATE, PATCH_RATE, 1] \n",
    "PADDING     = 'VALID'\n",
    "\n",
    "def compute_tile_patch_number(rates=[1,2,3]):\n",
    "    \"\"\" we expect that all tile share the same size \"\"\"\n",
    "    with tf.device('/CPU:0'):\n",
    "        aerial = tf.constant(imageio.imread(train_images[0]))\n",
    "        total  = 0\n",
    "        for r in rates:\n",
    "            patches = tf.image.extract_patches(images=tf.expand_dims(aerial, axis=0), sizes=SIZES, strides=STRIDES, rates=[1,r,r,1], padding=PADDING)            \n",
    "            total += patches.shape[1] * patches.shape[2]\n",
    "        return total\n",
    "\n",
    "# only patch at one scale are generated for INRIA\n",
    "TRAIN_PATCH_NUMBER = compute_tile_patch_number(rates=[1])\n",
    "VALID_PATCH_NUMBER = compute_tile_patch_number(rates=[1])\n",
    "TRAIN_PATCH_NUMBER, VALID_PATCH_NUMBER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SHAPE  = (256, 256, 3)\n",
    "PATCH_RESIZE = (128, 128)\n",
    "NUM_CLASSES  = 2\n",
    "BATCH_SIZE   = 50\n",
    "EPOCHS       = 50\n",
    "NUM_TRAIN_EXAMPLES = TRAIN_PATCH_NUMBER * len_train_images \n",
    "NUM_VAL_EXAMPLES   = VALID_PATCH_NUMBER * len_val_images \n",
    "LEARNING_RATE   = 0.0001\n",
    "TRAIN_STEPS_PER_EPOCH = int(np.ceil(NUM_TRAIN_EXAMPLES / float(BATCH_SIZE)))\n",
    "VAL_STEPS_PER_EPOCH   = int(np.ceil(NUM_VAL_EXAMPLES / float(BATCH_SIZE)))\n",
    "TRAIN_STEPS_PER_EPOCH, VAL_STEPS_PER_EPOCH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask to lavel conversion functions\n",
    "def mask2label(mask, colors=color_list,sparse=False):\n",
    "    label_list = []\n",
    "    if sparse:\n",
    "        for i,color in enumerate(colors):\n",
    "            # sparse encoding returned in tf.int32 to be able to use it in tf.gather as indices\n",
    "            label_list.append(i * tf.cast(tf.reduce_all(tf.equal(mask,color),axis=-1), dtype=tf.int32))\n",
    "            label = tf.add_n(label_list)\n",
    "\n",
    "    else:\n",
    "        for i,color in enumerate(colors):\n",
    "            label_list.append(tf.cast(tf.reduce_all(tf.equal(mask,color),axis=-1), dtype=tf.int8))\n",
    "            label = tf.stack(label_list, axis=-1)\n",
    "            # here is another way to return a sparse label from one hot encoded label\n",
    "            # for a strange reason, can't return tf.int8 as output_type\n",
    "            # label = mask2label(test_mask)\n",
    "            # sparse_label = mask2label(test_mask, sparse=True)\n",
    "            # arg_label = tf.argmax(label, axis=-1, output_type=tf.int32)\n",
    "            # so we cast it here back to tf.int8 (not needed after all)\n",
    "            # arg_label = tf.dtypes.cast(arg_label, tf.int8)\n",
    "            # checking that the two implementation return the same thing\n",
    "            # tf.reduce_all(tf.equal(arg_label, sparse_label)).numpy()            \n",
    "    return label                      \n",
    "\n",
    "def label2mask(sparse_label, colors=color_list):\n",
    "    # label should be a sparse encoding of the mask, not one-hot encoding\n",
    "    # TODO, add assertion to check sparse encoding (check last dimension = 1, not = number of classes)\n",
    "    mask = tf.gather(color_list, sparse_label)\n",
    "    mask = tf.dtypes.cast(mask, tf.float32)\n",
    "    return mask\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_show_pair(img_list, title=None, interpolation=None, **kwargs):\n",
    "    \"\"\"helper to display an image pairs side by side \"\"\"\n",
    "    # get a grid of axes in the figure\n",
    "    f, ax_list = plt.subplots(1, 2 ,figsize=(20,10))\n",
    "    for ax, img in zip(ax_list, img_list):\n",
    "        ax.imshow(img, interpolation=interpolation, **kwargs)\n",
    "        ax.axis('off')\n",
    "        if title:\n",
    "            ax.set_title(title)\n",
    "\n",
    "# utility function to display image pairs in a grid\n",
    "def my_show_grid(img_list, nrow=1, ncol=2, title=None, interpolation=None, **kwargs):\n",
    "    \"\"\"helper to display an image list in a grid \"\"\"\n",
    "    # here img_list is tuple\n",
    "    flatten_img_list = zip(*img_list)\n",
    "    # flatten the tuples\n",
    "    flatten_img_list = [np.squeeze(item) for sublist in flatten_img_list for item in sublist]\n",
    "    # get a grid of axes in the figure, make sure you have the correct count of axes\n",
    "    f, ax_list = plt.subplots(nrow, ncol ,figsize=(20,8), gridspec_kw = {'wspace':0.05, 'hspace':0.05})\n",
    "    ax_list = ax_list.flatten()\n",
    "    for ax, img in zip(ax_list, flatten_img_list):\n",
    "        ax.imshow(img, interpolation=interpolation, **kwargs)\n",
    "        ax.axis('off')\n",
    "        if title:\n",
    "            ax.set_title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timepipeline(ds, batch_size = 1, steps=1000):\n",
    "    \"\"\"Computes the throughput of the dataset iterations\"\"\"\n",
    "    start = time.time()\n",
    "    it = iter(ds)\n",
    "    for i in tqdm(range(steps)):\n",
    "        batch = next(it)\n",
    "    print()\n",
    "    end = time.time()\n",
    "    duration = end-start\n",
    "    print(\"batche size: {}\".format(batch_size))\n",
    "    print(\"{} batches: {} s\".format(steps, duration))\n",
    "    print(\"{:0.5f} Batches/s\".format(steps/duration))\n",
    "    print(\"{:0.5f} Images/s\".format(batch_size*steps/duration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patch Extraction:\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we are ready to make our final dataset\n",
    "def make_patches(aerial, mask):\n",
    "    with tf.device('/CPU:0'):\n",
    "        # first we generate the patches from the pair of aerial and mask images\n",
    "        patches = tf.image.extract_patches(images=[aerial, mask], sizes=SIZES, strides=STRIDES, rates=[1,1,1,1], padding=PADDING)\n",
    "        # at this point patches shape = [2, 23, 23, 196608]\n",
    "        # the image content is flattened, let's bring back the RGB channels\n",
    "        # patches = tf.reshape(patches, [2,289,512,512,3])\n",
    "        patch_nb = tf.shape(patches)[1] * tf.shape(patches)[2]\n",
    "        patches = tf.reshape(patches, [2, patch_nb, PATCH_SIZE, PATCH_SIZE, 3])\n",
    "        # lastly we want to permute the 0th and the 1st dimension\n",
    "        # so we end up with a list of pair of patches: aerial + mask, \n",
    "        # produced dim = [patch_number, 2, patch_size, patch_size, 3]\n",
    "        patches = tf.transpose(patches, perm=[1,0,2,3,4])\n",
    "        return patches\n",
    "\n",
    "# This variant of the function help create a dataset with patches at 3 scales (zooming effect)\n",
    "def make_patches_scaled(aerial, mask):\n",
    "    with tf.device('/CPU:0'):\n",
    "        # first we generate the patches from the pair of aerial and mask images\n",
    "        patches_rate1 = tf.image.extract_patches(images=[aerial, mask], sizes=SIZES, strides=STRIDES, rates=[1,1,1,1], padding=PADDING)\n",
    "        patches_rate2 = tf.image.extract_patches(images=[aerial, mask], sizes=SIZES, strides=STRIDES, rates=[1,2,2,1], padding=PADDING)\n",
    "        patches_rate3 = tf.image.extract_patches(images=[aerial, mask], sizes=SIZES, strides=STRIDES, rates=[1,3,3,1], padding=PADDING)\n",
    "        # at this point patches shape = [2, 23, 23, 196608]\n",
    "        # the image content is flattened, let's bring back the RGB channels\n",
    "        # patches = tf.reshape(patches, [2,289,512,512,3])\n",
    "\n",
    "        patch_number_rate1 = tf.shape(patches_rate1)[1] * tf.shape(patches_rate1)[2]\n",
    "        patch_number_rate2 = tf.shape(patches_rate2)[1] * tf.shape(patches_rate2)[2]\n",
    "        patch_number_rate3 = tf.shape(patches_rate3)[1] * tf.shape(patches_rate3)[2]\n",
    "        patches_rate1 = tf.reshape(patches_rate1, [2, patch_number_rate1, PATCH_SIZE, PATCH_SIZE, 3])\n",
    "        patches_rate2 = tf.reshape(patches_rate2, [2, patch_number_rate2, PATCH_SIZE, PATCH_SIZE, 3])\n",
    "        patches_rate3 = tf.reshape(patches_rate3, [2, patch_number_rate3, PATCH_SIZE, PATCH_SIZE, 3])\n",
    "        patches = tf.concat([patches_rate1,patches_rate2,patches_rate3],axis=1)\n",
    "        # lastly we want to permute the 0th and the 1st dimension\n",
    "        # so we end up with a list of pair of patches: aerial + mask, \n",
    "        # produced dim = [patch_number, 2, patch_size, patch_size, 3]\n",
    "        patches = tf.transpose(patches, perm=[1,0,2,3,4])\n",
    "        return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data augmentation methods\n",
    "def make_augmentation(pair_batch):\n",
    "    with tf.device('/CPU:0'):\n",
    "        # Spatial Transformation\n",
    "        do_flip    = tf.random.uniform([]) > 0.5\n",
    "        do_rot90   = tf.random.uniform([], maxval=4, dtype=tf.int32)\n",
    "        pair_batch = tf.cond(do_flip, lambda: tf.image.flip_left_right(pair_batch), lambda: pair_batch)\n",
    "        pair_batch = tf.image.rot90(pair_batch, do_rot90)\n",
    "        # rescale image from uint8 [0..255] to float32 [0..1]\n",
    "        pair_batch = tf.image.convert_image_dtype(pair_batch,tf.float32)\n",
    "        # temp solution, to be removed, used only if patches bigger than the model input size are generated \n",
    "        # Beware, tf.image.resize change dtype from uint8 to float32 while keeping [0..255] range\n",
    "        # pair_batch = tf.image.resize(pair_batch, PATCH_RESIZE, method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "        img = pair_batch[0]\n",
    "        # TODO: does per_image_standardization help get a better model ???\n",
    "        # img = tf.image.per_image_standardization(img)\n",
    "        \n",
    "        # color transformation\n",
    "        # TODO, remove comment after finding best values for color transformation\n",
    "#         img = tf.image.random_hue(img, 0.08)\n",
    "#         img = tf.image.random_saturation(img, 0.6, 1.6)\n",
    "#         img = tf.image.random_brightness(img, 0.05)\n",
    "#         img = tf.image.random_contrast(img, 0.7, 1.3)\n",
    "#         img = tf.clip_by_value(img, 0, 1)\n",
    "\n",
    "        mask = pair_batch[1]\n",
    "        mask = mask2label(mask, sparse=True)\n",
    "        # if only a particular class is of interest, enable following code\n",
    "        # example for buildings (class #1)\n",
    "        # mask = mask[:,:,1]\n",
    "        # mask = tf.expand_dims(mask, axis=-1)    \n",
    "        return img, mask\n",
    "\n",
    "# no augmentation for validation set\n",
    "def no_augmentation(pair_batch):\n",
    "    with tf.device('/CPU:0'):\n",
    "        # rescale image from uint8 [0..255] to float32 [0..1]\n",
    "        pair_batch = tf.image.convert_image_dtype(pair_batch,tf.float32)\n",
    "        # temp solution, to be removed, used only if patches bigger than the model input size are generated \n",
    "        # Beware, tf.image.resize change dtype from uint8 to float32 while keeping [0..255] range\n",
    "        # pair_batch = tf.image.resize(pair_batch, PATCH_RESIZE, method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "        img = pair_batch[0]\n",
    "        # TODO: does per_image_standardization help get a better model ???\n",
    "        # img = tf.image.per_image_standardization(img)\n",
    "        mask = pair_batch[1]\n",
    "        mask = mask2label(mask, sparse=True)\n",
    "        return img, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics Definition:\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_keras_history(keras_history):\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(131)\n",
    "    plt.plot(keras_history.history['Accuracy'], label='train')\n",
    "    plt.plot(keras_history.history['val_Accuracy'],label='validation')\n",
    "    plt.grid(axis='y')\n",
    "    plt.legend()\n",
    "    plt.title('Accuracy per epoch')\n",
    "    plt.subplot(132)\n",
    "    plt.plot(keras_history.history['mean_iou'], label='train')\n",
    "    plt.plot(keras_history.history['val_mean_iou'], label='validation')\n",
    "    plt.grid(axis='y')\n",
    "    plt.legend()\n",
    "    plt.title('Mean IoU per epoch')\n",
    "    plt.subplot(133)\n",
    "    plt.plot(keras_history.history['loss'], label='train')\n",
    "    plt.plot(keras_history.history['val_loss'], label='validation')\n",
    "    plt.grid(axis='y')\n",
    "    plt.legend()\n",
    "    plt.title('Loss per epoch')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cm(cm):\n",
    "    \"\"\" show the Normalized Confusion Matrix for all prediction classes \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(16,8))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "               yticks=np.arange(cm.shape[0]),\n",
    "               # ... and label them with the respective list entries\n",
    "               xticklabels=color_names, yticklabels=color_names,\n",
    "               title='Normalized Confusion matrix',\n",
    "               ylabel='True label',\n",
    "               xlabel='Predicted label')\n",
    "     # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",rotation_mode=\"anchor\")\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    #fmt = '.2f' if normalize else 'd'\n",
    "    fmt = '.2f'\n",
    "    thresh = tf.reduce_max(cm) / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt), ha=\"center\", va=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing IoU per class\n",
    "with tf.device('/CPU:0'):\n",
    "    def compute_batch_iou(model, patch_pairs):\n",
    "        pred = model.predict(patch_pairs[0])\n",
    "        pred = tf.argmax(pred, axis=-1, output_type=tf.int32)\n",
    "        label = patch_pairs[1]\n",
    "        total_cm = tf.math.confusion_matrix(tf.reshape(label, [-1]), tf.reshape(pred, [-1]))\n",
    "        sum_over_row = tf.reduce_sum(total_cm, axis=0)\n",
    "        sum_over_col = tf.reduce_sum(total_cm, axis=1)\n",
    "        true_positives = tf.linalg.diag_part(total_cm)\n",
    "        denominator = sum_over_row + sum_over_col - true_positives\n",
    "        num_valid_entries = tf.reduce_sum(tf.cast(tf.not_equal(denominator, 0), tf.float32))\n",
    "        iou = tf.math.divide_no_nan(tf.cast(true_positives, tf.float32), tf.cast(denominator, tf.float32))    \n",
    "        #return list(zip(iou.numpy().tolist(), color_names))\n",
    "        return pd.Series(data=iou, index=color_names).to_frame().T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "    def compute_dataset_cm(model, ds, steps, normalize=False):\n",
    "        total_cm = np.zeros((NUM_CLASSES,NUM_CLASSES))\n",
    "        for i, patch_pairs in tqdm(enumerate(ds.take(steps))):\n",
    "            pred = model.predict(patch_pairs[0])\n",
    "            pred = tf.argmax(pred, axis=-1, output_type=tf.int32)\n",
    "            label = patch_pairs[1]\n",
    "            total_cm += tf.math.confusion_matrix(tf.reshape(label, [-1]), tf.reshape(pred, [-1]))\n",
    "        # normalizing the confusing matrix\n",
    "        if normalize: \n",
    "            total_cm = total_cm / tf.reduce_sum(total_cm, axis=1, keepdims=True)\n",
    "        return total_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "    def compute_dataset_iou(model, ds, steps):\n",
    "        total_cm = compute_dataset_cm(model, ds, steps)\n",
    "        normalized_cm = total_cm / tf.reduce_sum(total_cm, axis=1, keepdims=True)\n",
    "        sum_over_row = tf.reduce_sum(total_cm, axis=0)\n",
    "        sum_over_col = tf.reduce_sum(total_cm, axis=1)\n",
    "        true_positives = tf.linalg.diag_part(total_cm)\n",
    "        denominator = sum_over_row + sum_over_col - true_positives\n",
    "        num_valid_entries = tf.reduce_sum(tf.cast(tf.not_equal(denominator, 0), tf.float32))\n",
    "        iou = tf.math.divide_no_nan(tf.cast(true_positives, tf.float32), tf.cast(denominator, tf.float32))\n",
    "        plot_cm(normalized_cm)\n",
    "        return pd.Series(data=iou, index=color_names).to_frame().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tile prediction functions (instant sliding window):\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "    def get_extract_pred_scatter(img,model):\n",
    "        H,W,C = img.shape\n",
    "        # patch_number \n",
    "        tile_PATCH_NUMBER = ((H - PATCH_SIZE)//PATCH_STRIDE + 1)*((W - PATCH_SIZE)//PATCH_STRIDE + 1)\n",
    "        # the indices trick to reconstruct the tile\n",
    "        x = tf.range(W)\n",
    "        y = tf.range(H)\n",
    "        x, y = tf.meshgrid(x, y)\n",
    "        indices = tf.stack([y, x], axis=-1)\n",
    "        # making patches, TensorShape([2, 17, 17, 786432])\n",
    "        img_patches = tf.image.extract_patches(images=tf.expand_dims(img, axis=0),     sizes=SIZES, strides=STRIDES, rates=RATES, padding=PADDING)\n",
    "        ind_patches = tf.image.extract_patches(images=tf.expand_dims(indices, axis=0), sizes=SIZES, strides=STRIDES, rates=RATES, padding=PADDING) \n",
    "        # squeezing the shape (removing dimension of size 1)\n",
    "        img_patches = tf.squeeze(img_patches)\n",
    "        ind_patches = tf.squeeze(ind_patches)\n",
    "        # reshaping\n",
    "        img_patches = tf.reshape(img_patches, [tile_PATCH_NUMBER, PATCH_SIZE, PATCH_SIZE, C])\n",
    "        ind_patches = tf.reshape(ind_patches, [tile_PATCH_NUMBER, PATCH_SIZE, PATCH_SIZE, 2])\n",
    "        # Now predict\n",
    "        pred_patches = model.predict(img_patches, batch_size=BATCH_SIZE)\n",
    "        # stitch together the patch summing the overlapping patches probabilities\n",
    "        pred_tile    = tf.scatter_nd(indices=ind_patches, updates=pred_patches, shape=(H,W,NUM_CLASSES))\n",
    "        return pred_tile\n",
    "\n",
    "    def get_tile_prediction(tile_path, model, from_disk=True):\n",
    "        # reading the tile content\n",
    "        if from_disk:\n",
    "            img = imageio.imread(tile_path)            \n",
    "        else:\n",
    "            img = tile_path\n",
    "        img = tf.image.convert_image_dtype(img,tf.float32)\n",
    "        pred_tile = get_extract_pred_scatter(img,model)    \n",
    "        pred_tile    = tf.argmax(pred_tile, axis=-1, output_type=tf.int32)\n",
    "        pred_tile    = label2mask(pred_tile)\n",
    "        return pred_tile\n",
    "\n",
    "    def get_tile_tta_pred(tile_path, model, from_disk=True):\n",
    "        \"\"\" test time augmentation prediction \"\"\"\n",
    "        # reading the tile content\n",
    "        if from_disk:\n",
    "            img = imageio.imread(tile_path)\n",
    "        else:\n",
    "            img = tile_path\n",
    "        img = tf.image.convert_image_dtype(img,tf.float32)\n",
    "        H,W,C = img.shape\n",
    "        pred_tile = tf.zeros(shape=(H,W,NUM_CLASSES))\n",
    "        for i in tqdm(tf.range(4)):\n",
    "            rot_img = tf.image.rot90(img,k=i)\n",
    "            pred_tmp = get_extract_pred_scatter(rot_img,model)\n",
    "            pred_tile += tf.image.rot90(pred_tmp,k=-i)\n",
    "        img = tf.image.flip_left_right(img)\n",
    "        for i in tqdm(tf.range(4)):\n",
    "            rot_img = tf.image.rot90(img,k=i)\n",
    "            pred_tmp = get_extract_pred_scatter(rot_img,model)\n",
    "            pred_tile += tf.image.flip_left_right(tf.image.rot90(pred_tmp,k=-i))\n",
    "        pred_tile    = tf.argmax(pred_tile, axis=-1, output_type=tf.int32)\n",
    "        pred_tile    = label2mask(pred_tile)\n",
    "        return pred_tile    \n",
    "\n",
    "    def get_tile_cm(y_true, y_pred, normalize=False):\n",
    "        y_true = mask2label(y_true, sparse=True)\n",
    "        y_pred = mask2label(y_pred, sparse=True)\n",
    "        tile_cm = tf.math.confusion_matrix(tf.reshape(y_true, [-1]), tf.reshape(y_pred, [-1]))\n",
    "        tile_cm = tf.cast(tile_cm, tf.float32)\n",
    "        if normalize:\n",
    "            tile_cm = tile_cm / tf.reduce_sum(tile_cm, axis=1, keepdims=True)\n",
    "        return tile_cm\n",
    "\n",
    "    def get_tile_iou(y_true, y_pred):\n",
    "        tile_cm = get_tile_cm(y_true, y_pred)\n",
    "        sum_over_row = tf.reduce_sum(tile_cm, axis=0)\n",
    "        sum_over_col = tf.reduce_sum(tile_cm, axis=1)\n",
    "        true_positives = tf.linalg.diag_part(tile_cm)\n",
    "        denominator = sum_over_row + sum_over_col - true_positives\n",
    "        num_valid_entries = tf.reduce_sum(tf.cast(tf.not_equal(denominator, 0), tf.float32))\n",
    "        iou = tf.math.divide_no_nan(tf.cast(true_positives, tf.float32), tf.cast(denominator, tf.float32))\n",
    "        return pd.Series(data=iou, index=color_names).to_frame().T    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_dataset wrapper function:\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "    def get_image(srcpath, refpath):\n",
    "        # black and white color list, only useful for binary segmentation\n",
    "        color_list_local  = [[0, 0, 0], [255, 255, 255]]\n",
    "        srcpath = srcpath.numpy().decode(\"utf-8\")\n",
    "        refpath = refpath.numpy().decode(\"utf-8\")\n",
    "        src = imageio.imread(srcpath)\n",
    "        ref = imageio.imread(refpath)\n",
    "        # for binary segmentation, mask is only grayscale, not RGB\n",
    "        # convert it here to 3 (RGB) channels to avoid breaking previous code\n",
    "        if (len(ref.shape) == 2):\n",
    "            ref = ref // 255\n",
    "            ref = tf.cast(ref, tf.int32)\n",
    "            ref = tf.gather(color_list_local, ref)\n",
    "            ref = tf.cast(ref, tf.uint8)\n",
    "        return src,ref\n",
    "    def tf_get_image(srcpath, refpath):\n",
    "        src,ref = tf.py_function(func=get_image, inp=[srcpath, refpath], Tout=[tf.uint8,tf.uint8])\n",
    "        return src,ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "    def get_dataset(img_dir, mask_dir, batch_size, shuffle=True, repeat=False, augment=True, scaled=True, cachefile=None):\n",
    "        images_path  = Path(img_dir)\n",
    "        masks_path   = Path(mask_dir)\n",
    "        images_files = sorted([str(x) for x in images_path.iterdir() if x.is_file() and x.suffix == '.tif'])\n",
    "        masks_files  = sorted([str(x) for x in masks_path.iterdir()  if x.is_file() and x.suffix == '.tif'])\n",
    "\n",
    "        # checking the equality of the two list lenghts\n",
    "        assert len(images_files) == len(masks_files), \" (Error) The Aerial image count does not match Mask counts!\"\n",
    "        len_images = len(images_files)\n",
    "        \n",
    "        if cachefile:\n",
    "            cache_file_name = '/tmp/' + cachefile\n",
    "            # start with a fresh cache file\n",
    "            for f in list(Path('/tmp').glob(cachefile + '*')):\n",
    "                print('Removing old cache file: {}'.format(f))\n",
    "                f.unlink()\n",
    "        else:\n",
    "            cache_file_name = ''\n",
    "\n",
    "        # First strategy to get images files dataset\n",
    "        images_ds = tf.data.Dataset.from_tensor_slices(images_files)\n",
    "        masks_ds = tf.data.Dataset.from_tensor_slices(masks_files)\n",
    "\n",
    "        # second strategy\n",
    "        # maybe this strategy does not garantee the order of the lists\n",
    "        #images_ds = tf.data.Dataset.list_files(str(images_path/'*.tif'),shuffle=False) # commented out\n",
    "        #masks_ds  = tf.data.Dataset.list_files(str(masks_path/'*.tif'),shuffle=False)  # commented ou\n",
    "        \n",
    "        # zipping the aerial and mask dataset\n",
    "        patch_pair_ds = tf.data.Dataset.zip((images_ds, masks_ds))\n",
    "        if shuffle:\n",
    "            patch_pair_ds = patch_pair_ds.shuffle(len_images) \n",
    "        patch_pair_ds = patch_pair_ds.map(tf_get_image, num_parallel_calls= AUTOTUNE).prefetch(AUTOTUNE)\n",
    "\n",
    "        # start making patches\n",
    "        if scaled:\n",
    "            patch_pair_ds = patch_pair_ds.map(make_patches_scaled, num_parallel_calls= AUTOTUNE)\n",
    "        else:\n",
    "            patch_pair_ds = patch_pair_ds.map(make_patches, num_parallel_calls= AUTOTUNE)\n",
    "        patch_pair_ds = patch_pair_ds.apply(tf.data.Dataset.unbatch).cache(filename=cache_file_name)\n",
    "        if augment:\n",
    "            patch_pair_ds = patch_pair_ds.map(make_augmentation, num_parallel_calls= AUTOTUNE).prefetch(AUTOTUNE)\n",
    "        else:\n",
    "            patch_pair_ds = patch_pair_ds.map(no_augmentation, num_parallel_calls= AUTOTUNE).prefetch(AUTOTUNE)\n",
    "        # repeat is not necessary as keras fit method will use epochs parameter for repetition\n",
    "        if repeat:\n",
    "            patch_pair_ds = patch_pair_ds.repeat()\n",
    "        if shuffle:\n",
    "            patch_pair_ds = patch_pair_ds.shuffle(600) # maybe this needs a new parameter?\n",
    "        patch_pair_ds = patch_pair_ds.batch(batch_size)\n",
    "        patch_pair_ds = patch_pair_ds.prefetch(AUTOTUNE)\n",
    "        return patch_pair_ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Validation Datasets:\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = get_dataset(img_dir=train_img_dir, mask_dir=train_mask_dir, batch_size=BATCH_SIZE, shuffle=True, repeat=True, augment=True, scaled=False, cachefile=None)\n",
    "val_ds   = get_dataset(img_dir=val_img_dir, mask_dir=val_mask_dir,batch_size=BATCH_SIZE, shuffle=False, repeat=True, augment=False, scaled=False, cachefile=None)\n",
    "print(\"train patch number = {}, valid patch number = {}\".format(TRAIN_PATCH_NUMBER, VALID_PATCH_NUMBER))\n",
    "print(\"train steps = {}, valid steps = {}\".format(TRAIN_STEPS_PER_EPOCH, VAL_STEPS_PER_EPOCH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance of get_dataset wrapper function:\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = get_dataset(img_dir=train_img_dir, mask_dir=train_mask_dir, batch_size=BATCH_SIZE, shuffle=True, repeat=True, augment=True, scaled=False, cachefile=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timepipeline(train_ds, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = get_dataset(img_dir=val_img_dir, mask_dir=val_mask_dir, batch_size=BATCH_SIZE, shuffle=False, repeat=True, augment=False, scaled=False, cachefile=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timepipeline(val_ds, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator = iter(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting next pair of aerial image and its associated mask\n",
    "patch_pairs = next(train_iterator)\n",
    "# display images\n",
    "my_show_grid(patch_pairs, nrow=4, ncol=10)\n",
    "# plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimenting with options\n",
    "# aggregator = tf.data.experimental.StatsAggregator()\n",
    "\n",
    "# options = tf.data.Options()\n",
    "# options.experimental_stats.aggregator = aggregator\n",
    "# options.experimental_stats.latency_all_edges = True\n",
    "# pair_ds = pair_ds.with_options(options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Keras Functional API:\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(input_tensor, num_filters, dilation=1, residual=False):\n",
    "    x = layers.Conv2D(num_filters, (3,3), activation='relu', padding='same', dilation_rate=dilation, kernel_initializer='he_normal')(input_tensor)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv2D(num_filters, (3,3), activation='relu', padding='same', dilation_rate=dilation, kernel_initializer='he_normal')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    if residual:\n",
    "        x  = layers.Concatenate()([x, input_tensor])\n",
    "    return x\n",
    "\n",
    "def bottleneck_block(input_tensor, num_filters, mode='parallel'):\n",
    "    if mode == 'serial':\n",
    "        dilated1  = layers.Conv2D(num_filters, (3,3), activation='relu', padding='same', dilation_rate=1, kernel_initializer='he_normal')(input_tensor)\n",
    "        dilated1  = layers.BatchNormalization()(dilated1)\n",
    "        dilated2  = layers.Conv2D(num_filters, (3,3), activation='relu', padding='same', dilation_rate=2, kernel_initializer='he_normal')(dilated1)\n",
    "        dilated2  = layers.BatchNormalization()(dilated2)\n",
    "        dilated4  = layers.Conv2D(num_filters, (3,3), activation='relu', padding='same', dilation_rate=4, kernel_initializer='he_normal')(dilated2)\n",
    "        dilated4  = layers.BatchNormalization()(dilated4)\n",
    "        dilated8  = layers.Conv2D(num_filters, (3,3), activation='relu', padding='same', dilation_rate=8, kernel_initializer='he_normal')(dilated4)\n",
    "        dilated8  = layers.BatchNormalization()(dilated8)\n",
    "        x  = layers.Concatenate()([dilated1, dilated2, dilated4, dilated8])\n",
    "    elif mode == 'parallel':\n",
    "        dilated1 = conv_block(input_tensor, num_filters, dilation=1)\n",
    "        dilated2 = conv_block(input_tensor, num_filters, dilation=2)\n",
    "        dilated4 = conv_block(input_tensor, num_filters, dilation=4)\n",
    "        dilated8 = conv_block(input_tensor, num_filters, dilation=8)\n",
    "        x  = layers.Concatenate()([dilated1, dilated2, dilated4, dilated8])\n",
    "    else:\n",
    "        x = conv_block(input_tensor, num_filters, dilation=1, residual=True)\n",
    "    return x\n",
    "\n",
    "def encoder_block(input_tensor, num_filters, residual=True):\n",
    "    encoder = conv_block(input_tensor, num_filters, residual=residual)\n",
    "    encoder_pool = layers.MaxPooling2D((2,2), strides=(2,2))(encoder)\n",
    "    return encoder_pool, encoder\n",
    "\n",
    "def decoder_block(input_tensor, concat_tensor, num_filters, residual=True):\n",
    "    x = layers.Conv2DTranspose(num_filters, (2,2), strides=(2,2), padding='same', kernel_initializer='he_normal')(input_tensor)\n",
    "    x = layers.concatenate([concat_tensor, x], axis=-1)\n",
    "    shortcut = x\n",
    "    x = layers.Conv2D(num_filters, (3,3), activation='relu', padding='same', kernel_initializer='he_normal')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv2D(num_filters, (3,3), activation='relu', padding='same', kernel_initializer='he_normal')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    if residual:\n",
    "        x  = layers.Concatenate()([x, shortcut])\n",
    "    return x\n",
    "\n",
    "def encoder_proc(x, filters=32, n_block=4, residual=True):\n",
    "    skip = []\n",
    "    for i in range(n_block):\n",
    "        x, x_skip = encoder_block(x, filters * 2**i, residual=residual)\n",
    "        skip.append(x_skip)\n",
    "    return x, skip\n",
    "\n",
    "def decoder_proc(x, skip, filters=32, n_block=4, residual=True):\n",
    "    for i in reversed(range(n_block)):\n",
    "        x = decoder_block(x, skip[i], filters * 2**i, residual=residual)\n",
    "    return x\n",
    "\n",
    "def get_unet_model(input_shape, filters=32, n_block=4, n_class=6, mode='parallel', residual=True):\n",
    "    if n_class == 2:\n",
    "        final_activation = 'sigmoid'\n",
    "    elif n_class > 2:\n",
    "        final_activation = 'softmax'\n",
    "    final_activation = 'softmax'\n",
    "    inputs = layers.Input(input_shape)    \n",
    "    enc, skip = encoder_proc(inputs, filters, n_block, residual=residual)\n",
    "    center = bottleneck_block(enc, num_filters=filters * 2**n_block, mode=mode)\n",
    "    dec = decoder_proc(center, skip, filters, n_block, residual=residual)\n",
    "    classify = layers.Conv2D(n_class, (1, 1), activation=final_activation)(dec)\n",
    "    model = models.Model(inputs=inputs, outputs=classify)\n",
    "    return model    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Rate Finder\n",
    "class LearningRateFinder:\n",
    "    def __init__(self, model, stopFactor=10, beta=0.98):\n",
    "        # store the model, stop factor, and beta value (for computing a smoothed, average loss)\n",
    "        self.model = model\n",
    "        self.stopFactor = stopFactor\n",
    "        self.beta = beta\n",
    "\n",
    "        # initialize our list of learning rates and losses, respectively\n",
    "        self.lrs = []\n",
    "        self.losses = []\n",
    "\n",
    "        # initialize our learning rate multiplier, average loss, best loss found thus far, current batch number, and weights file\n",
    "        self.lrMult = 1\n",
    "        self.avgLoss = 0\n",
    "        self.bestLoss = 1e9\n",
    "        self.batchNum = 0\n",
    "        self.weightsFile = None\n",
    "\n",
    "    def reset(self):\n",
    "        # re-initialize all variables from our constructor\n",
    "        self.lrs = []\n",
    "        self.losses = []\n",
    "        self.lrMult = 1\n",
    "        self.avgLoss = 0\n",
    "        self.bestLoss = 1e9\n",
    "        self.batchNum = 0\n",
    "        self.weightsFile = None   \n",
    " \n",
    "    def on_batch_end(self, batch, logs):\n",
    "        # grab the current learning rate and add log it to the list of learning rates that we've tried\n",
    "        lr = K.get_value(self.model.optimizer.lr)\n",
    "        self.lrs.append(lr)\n",
    "\n",
    "        # grab the loss at the end of this batch, increment the total number of batches processed, compute the average average\n",
    "        # loss, smooth it, and update the losses list with the smoothed value\n",
    "        l = logs[\"loss\"]\n",
    "        self.batchNum += 1\n",
    "        self.avgLoss = (self.beta * self.avgLoss) + ((1 - self.beta) * l)\n",
    "        smooth = self.avgLoss / (1 - (self.beta ** self.batchNum))\n",
    "        self.losses.append(smooth)\n",
    "\n",
    "        # compute the maximum loss stopping factor value\n",
    "        stopLoss = self.stopFactor * self.bestLoss\n",
    "\n",
    "        # check to see whether the loss has grown too large\n",
    "        if self.batchNum > 1 and smooth > stopLoss:\n",
    "            # stop returning and return from the method\n",
    "            self.model.stop_training = True\n",
    "            return\n",
    "\n",
    "        # check to see if the best loss should be updated\n",
    "        if self.batchNum == 1 or smooth < self.bestLoss:\n",
    "            self.bestLoss = smooth\n",
    "\n",
    "        # increase the learning rate\n",
    "        lr *= self.lrMult\n",
    "        K.set_value(self.model.optimizer.lr, lr)\n",
    "\n",
    "    def find(self, trainData, startLR, endLR, epochs=1, stepsPerEpoch=None, verbose=1):\n",
    "        # reset our class-specific variables\n",
    "        self.reset()\n",
    "\n",
    "        # compute the total number of batch updates that will take place while we are attempting to find a good starting\n",
    "        # learning rate\n",
    "        numBatchUpdates = epochs * stepsPerEpoch\n",
    "\n",
    "        # derive the learning rate multiplier based on the ending learning rate, starting learning rate, and total number of\n",
    "        # batch updates\n",
    "        self.lrMult = (endLR / startLR) ** (1.0 / numBatchUpdates)\n",
    "\n",
    "        # create a temporary file path for the model weights and then save the weights (so we can reset the weights when we\n",
    "        # are done)\n",
    "        self.weightsFile = tempfile.mkstemp()[1]\n",
    "        self.model.save_weights(self.weightsFile)\n",
    "\n",
    "        # grab the *original* learning rate (so we can reset it later), and then set the *starting* learning rate\n",
    "        origLR = K.get_value(self.model.optimizer.lr)\n",
    "        K.set_value(self.model.optimizer.lr, startLR)\n",
    "\n",
    "        # construct a callback that will be called at the end of each batch, enabling us to increase our learning rate as training\n",
    "        # progresses\n",
    "        callback = LambdaCallback(on_batch_end=lambda batch, logs: self.on_batch_end(batch, logs))\n",
    "        self.model.fit(trainData, steps_per_epoch=stepsPerEpoch, epochs=epochs, verbose=verbose, callbacks=[callback])\n",
    "\n",
    "\n",
    "        # restore the original model weights and learning rate\n",
    "        self.model.load_weights(self.weightsFile)\n",
    "        K.set_value(self.model.optimizer.lr, origLR)\n",
    "\n",
    "    def plot_loss(self, skipBegin=2, skipEnd=1, title=\"\"):\n",
    "        # grab the learning rate and losses values to plot\n",
    "        lrs = self.lrs[skipBegin:-skipEnd]\n",
    "        losses = self.losses[skipBegin:-skipEnd]\n",
    "\n",
    "        # plot the learning rate vs. loss   \n",
    "        fig, ax = plt.subplots(figsize=(15,6))\n",
    "        ax.plot(lrs, losses)\n",
    "        ax.set_xscale(\"log\")\n",
    "        ax.xaxis.set_major_locator(LogLocator(base=10., numticks=15, subs=range(10)))\n",
    "        ax.set_xlabel(\"Learning Rate (Log Scale)\")\n",
    "        ax.set_ylabel(\"Loss\")\n",
    "        ax.grid(True) \n",
    "        # if the title is not empty, add it to the plot\n",
    "        if title != \"\":\n",
    "            plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Cycle Policy training\n",
    "# inspired by https://github.com/shivam-agarwal-17/keras-one-cycle-policy/blob/master/one_cycle_lr/one_cycle_scheduler.py\n",
    "class ParamScheduler:\n",
    "    def __init__(self, start, end, num_iter):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.num_iter = num_iter\n",
    "        self.idx = -1\n",
    "        \n",
    "    def func(self, start_val, end_val, pct):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def step(self):\n",
    "        self.idx+=1\n",
    "        return self.func(self.start, self.end, self.idx/self.num_iter)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.idx=-1\n",
    "        \n",
    "    def is_complete(self):\n",
    "        return self.idx >= self.num_iter\n",
    "\n",
    "class LinearScheduler(ParamScheduler):\n",
    "    \n",
    "    def func(self, start_val, end_val, pct):\n",
    "        return start_val + pct * (end_val - start_val)\n",
    "    \n",
    "class CosineScheduler(ParamScheduler):\n",
    "    \n",
    "    def func(self, start_val, end_val, pct):\n",
    "        cos_out = np.cos(np.pi * pct) + 1\n",
    "        return end_val + (start_val - end_val)/2 * cos_out\n",
    "\n",
    "\n",
    "class OneCycleScheduler(Callback):\n",
    "    \n",
    "    def __init__(self, max_lr, momentums=(0.95,0.80), start_div=25., pct_start=0.3, verbose=True, sched=CosineScheduler, end_div=25e3):\n",
    "        self.max_lr, self.momentums, self.start_div,self.pct_start, self.verbose, self.sched, self.end_div = max_lr, momentums, start_div, pct_start, verbose, sched, end_div\n",
    "        self.logs = {}\n",
    "        \n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.num_epochs = self.params['epochs']\n",
    "        self.steps_per_epoch = self.params['steps']\n",
    "        start_lr   = self.max_lr / self.start_div\n",
    "        end_lr     = self.max_lr / self.end_div\n",
    "        num_iter   = self.num_epochs * self.steps_per_epoch\n",
    "        num_iter_1 = int(self.pct_start*num_iter)\n",
    "        num_iter_2 = num_iter - num_iter_1\n",
    "        self.lr_scheds = (self.sched(start_lr, self.max_lr, num_iter_1), self.sched(self.max_lr, end_lr, num_iter_2))\n",
    "        self.momentum_scheds = (self.sched(self.momentums[0], self.momentums[1], num_iter_1), self.sched(self.momentums[1], self.momentums[0], num_iter_2))\n",
    "        self.sched_idx = 0\n",
    "        self.optimizer_params_step()   \n",
    "        \n",
    "    def optimizer_params_step(self):\n",
    "        next_lr = self.lr_scheds[self.sched_idx].step()\n",
    "        next_momentum = self.momentum_scheds[self.sched_idx].step()\n",
    "        \n",
    "        # add to logs\n",
    "        self.logs.setdefault('lr', []).append(next_lr)\n",
    "        self.logs.setdefault('momentum', []).append(next_momentum)\n",
    "        \n",
    "        # update optimizer params\n",
    "        K.set_value(self.model.optimizer.lr, next_lr)\n",
    "        if hasattr(self.model.optimizer, 'momentum'):\n",
    "            K.set_value(self.model.optimizer.momentum, next_momentum)\n",
    "        \n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        if self.sched_idx >= len(self.lr_scheds):\n",
    "            self.model.stop_training=True\n",
    "            return\n",
    "        self.optimizer_params_step()\n",
    "        if self.lr_scheds[self.sched_idx].is_complete():\n",
    "            self.sched_idx += 1\n",
    "            \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if self.verbose:\n",
    "            if hasattr(self.model.optimizer, 'momentum'):\n",
    "                print(\" - OneCycleScheduler, lr: {:.7f}, momentum: {:.7f}\".format(self.logs['lr'][-1], self.logs['momentum'][-1]))\n",
    "            else:\n",
    "                print(\" - OneCycleScheduler, lr: {:.7f}\".format(self.logs['lr'][-1]))\n",
    "            \n",
    "        if epoch >= self.num_epochs:\n",
    "            self.model.stop_training=True\n",
    "            return\n",
    "        \n",
    "    def plot_lr(self, show_momentums=True):\n",
    "        plt.figure(figsize=(20,5))\n",
    "        if hasattr(self.model.optimizer, 'momentum') and show_momentums:\n",
    "            plt.subplot(131)\n",
    "            plt.plot(self.logs['lr'])\n",
    "            plt.ylabel('learning rate')\n",
    "            plt.xlabel('iteration') \n",
    "            plt.grid(True, linestyle=\"--\")\n",
    "\n",
    "            plt.subplot(132)\n",
    "            plt.plot(self.logs['lr'])\n",
    "            plt.yscale(\"log\")\n",
    "            plt.ylabel('learning rate (log)')\n",
    "            plt.xlabel('iteration')\n",
    "            plt.grid(True, linestyle=\"--\")\n",
    "\n",
    "            plt.subplot(133)\n",
    "            plt.plot(self.logs['momentum'])\n",
    "            plt.ylabel('momentum')\n",
    "            plt.xlabel('iteration')\n",
    "            plt.grid(True, linestyle=\"--\")\n",
    "        else:\n",
    "            plt.subplot(121)\n",
    "            plt.plot(self.logs['lr'])\n",
    "            plt.ylabel('learning rate')\n",
    "            plt.xlabel('iteration')\n",
    "            plt.grid(True, linestyle=\"--\")\n",
    "\n",
    "            plt.subplot(122)\n",
    "            plt.plot(self.logs['lr'])\n",
    "            plt.yscale(\"log\")\n",
    "            plt.ylabel('learning rate (Log scale)')\n",
    "            plt.xlabel('iteration')\n",
    "            plt.grid(True, linestyle=\"--\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fixing MeanIoU\n",
    "class CategoricalMeanIoU(tf.keras.metrics.MeanIoU):\n",
    "\n",
    "    def __init__(self, name='mean_iou', **kwargs):\n",
    "        super(CategoricalMeanIoU, self).__init__(name=name, **kwargs)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred = tf.argmax(y_pred, axis=-1)\n",
    "        return super(CategoricalMeanIoU, self).update_state(y_true=y_true, y_pred=y_pred, sample_weight=sample_weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_distance(y_true, y_pred):\n",
    "    \"\"\"Jaccard distance for semantic segmentation.\n",
    "\n",
    "    Also known as the intersection-over-union loss.\n",
    "\n",
    "    This loss is useful when you have unbalanced numbers of pixels within an image\n",
    "    because it gives all classes equal weight. However, it is not the defacto\n",
    "    standard for image segmentation.\n",
    "\n",
    "    For example, assume you are trying to predict if\n",
    "    each pixel is cat, dog, or background.\n",
    "    You have 80% background pixels, 10% dog, and 10% cat.\n",
    "    If the model predicts 100% background\n",
    "    should it be be 80% right (as with categorical cross entropy)\n",
    "    or 30% (with this loss)?\n",
    "\n",
    "    The loss has been modified to have a smooth gradient as it converges on zero.\n",
    "    This has been shifted so it converges on 0 and is smoothed to avoid exploding\n",
    "    or disappearing gradient.\n",
    "\n",
    "    Jaccard = (|X & Y|)/ (|X|+ |Y| - |X & Y|)\n",
    "            = sum(|A*B|)/(sum(|A|)+sum(|B|)-sum(|A*B|))\n",
    "\n",
    "    # Arguments\n",
    "        y_true: The ground truth tensor.\n",
    "        y_pred: The predicted tensor\n",
    "        smooth: Smoothing factor. Default is 100.\n",
    "\n",
    "    # Returns\n",
    "        The Jaccard distance between the two tensors.\n",
    "\n",
    "    # References\n",
    "        - [What is a good evaluation measure for semantic segmentation?](\n",
    "           http://www.bmva.org/bmvc/2013/Papers/paper0032/paper0032.pdf)\n",
    "\n",
    "    \"\"\"\n",
    "    smooth=1\n",
    "    y_true = tf.one_hot(tf.cast(y_true,tf.int32), NUM_CLASSES)\n",
    "#     This implementation is strange, does computing IoU with last axis correct ?\n",
    "#     intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "#     sum_ = K.sum(K.abs(y_true) + K.abs(y_pred), axis=-1)\n",
    "#     jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "#     return (1 - jac) * smooth\n",
    "\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=(0,1,2))\n",
    "    sum_ = K.sum(K.abs(y_true) + K.abs(y_pred), axis=(0,1,2))\n",
    "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "    return tf.reduce_mean((1 - jac))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_focal_loss(gamma=2., alpha=.25):\n",
    "    \"\"\"\n",
    "    Binary form of focal loss.\n",
    "      FL(p_t) = -alpha * (1 - p_t)**gamma * log(p_t)\n",
    "      where p = sigmoid(x), p_t = p or 1 - p depending on if the label is 1 or 0, respectively.\n",
    "    References:\n",
    "        https://arxiv.org/pdf/1708.02002.pdf\n",
    "    Usage:\n",
    "     model.compile(loss=[binary_focal_loss(alpha=.25, gamma=2)], metrics=[\"accuracy\"], optimizer=adam)\n",
    "    \"\"\"\n",
    "    def binary_focal_loss_fixed(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        :param y_true: A tensor of the same shape as `y_pred`\n",
    "        :param y_pred:  A tensor resulting from a sigmoid\n",
    "        :return: Output tensor.\n",
    "        \"\"\"\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "\n",
    "        epsilon = K.epsilon()\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        pt_1 = K.clip(pt_1, epsilon, 1. - epsilon)\n",
    "        pt_0 = K.clip(pt_0, epsilon, 1. - epsilon)\n",
    "\n",
    "        return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1)) \\\n",
    "               -K.sum((1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0))\n",
    "\n",
    "    return binary_focal_loss_fixed\n",
    "\n",
    "def categorical_focal_loss(gamma=2., alpha=.25):\n",
    "    \"\"\"\n",
    "    Softmax version of focal loss.\n",
    "           m\n",
    "      FL =   -alpha * (1 - p_o,c)^gamma * y_o,c * log(p_o,c)\n",
    "          c=1\n",
    "      where m = number of classes, c = class and o = observation\n",
    "    Parameters:\n",
    "      alpha -- the same as weighing factor in balanced cross entropy\n",
    "      gamma -- focusing parameter for modulating factor (1-p)\n",
    "    Default value:\n",
    "      gamma -- 2.0 as mentioned in the paper\n",
    "      alpha -- 0.25 as mentioned in the paper\n",
    "    References:\n",
    "        Official paper: https://arxiv.org/pdf/1708.02002.pdf\n",
    "        https://www.tensorflow.org/api_docs/python/tf/keras/backend/categorical_crossentropy\n",
    "    Usage:\n",
    "     model.compile(loss=[categorical_focal_loss(alpha=.25, gamma=2)], metrics=[\"accuracy\"], optimizer=adam)\n",
    "    \"\"\"\n",
    "    def categorical_focal_loss_fixed(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        :param y_true: A tensor of the same shape as `y_pred`\n",
    "        :param y_pred: A tensor resulting from a softmax\n",
    "        :return: Output tensor.\n",
    "        \"\"\"\n",
    "\n",
    "        # Scale predictions so that the class probas of each sample sum to 1 (not needed as last layer is softmax)\n",
    "#         y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
    "        y_true = tf.one_hot(tf.cast(y_true,tf.int32), NUM_CLASSES)\n",
    "\n",
    "        # Clip the prediction value to prevent NaN's and Inf's\n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "\n",
    "        # Calculate Cross Entropy\n",
    "        cross_entropy = -y_true * K.log(y_pred)\n",
    "\n",
    "        # Calculate Focal Loss\n",
    "        loss = alpha * K.pow(1 - y_pred, gamma) * cross_entropy\n",
    "\n",
    "        # Sum the losses in mini_batch\n",
    "        return K.sum(loss, axis=1)\n",
    "\n",
    "    return categorical_focal_loss_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_unet_model(MODEL_SHAPE, filters=32, n_block=4, n_class=6, mode='serial', residual=True)\n",
    "# if one wants to resets the weights of the model after training, use this lambda method\n",
    "reset_weights = model.get_weights()\n",
    "resetModel = lambda m: m.set_weights(reset_weights)\n",
    "# usage: resetModel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()\n",
    "# old way to declare a UNET model\n",
    "# inputs = layers.Input(shape=PATCH_SHAPE)\n",
    "# # 512\n",
    "\n",
    "# encoder0_pool, encoder0 = encoder_block(inputs, 32)\n",
    "# # 256\n",
    "\n",
    "# encoder1_pool, encoder1 = encoder_block(encoder0_pool, 64)\n",
    "# # 128\n",
    "\n",
    "# encoder2_pool, encoder2 = encoder_block(encoder1_pool, 128)\n",
    "# # 64\n",
    "\n",
    "# encoder3_pool, encoder3 = encoder_block(encoder2_pool, 256)\n",
    "# # 32\n",
    "\n",
    "# # encoder4_pool, encoder4 = encoder_block(encoder3_pool, 512)\n",
    "# # 16\n",
    "\n",
    "# center = bottleneck_block(encoder3_pool, 512, mode='parallel')\n",
    "# # center\n",
    "\n",
    "# # decoder4 = decoder_block(center, encoder4, 512)\n",
    "# # 32\n",
    "\n",
    "# decoder3 = decoder_block(center, encoder3, 256)\n",
    "# # 64\n",
    "\n",
    "# decoder2 = decoder_block(decoder3, encoder2, 128)\n",
    "# # 128\n",
    "\n",
    "# decoder1 = decoder_block(decoder2, encoder1, 64)\n",
    "# # 256\n",
    "\n",
    "# decoder0 = decoder_block(decoder1, encoder0, 32)\n",
    "# # 512\n",
    "\n",
    "# outputs = layers.Conv2D(NUM_CLASSES, (1,1), activation='softmax')(decoder0)\n",
    "# model = models.Model(inputs=[inputs], outputs=[outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"unet_input256_noScale_skip4_Residual_serialCenter_sgd_cross_entropy_bs50_ocpCB\"\n",
    "sgd_optim = tf.keras.optimizers.SGD(lr=0.1, nesterov=True)\n",
    "adam_optim = tf.keras.optimizers.Adam(lr=1e-3)\n",
    "acc_metric = tf.keras.metrics.SparseCategoricalAccuracy(name='Accuracy')\n",
    "iou_metric = tf.keras.metrics.MeanIoU(num_classes=NUM_CLASSES)\n",
    "fixed_iou  = CategoricalMeanIoU(num_classes=NUM_CLASSES)\n",
    "model.compile(optimizer=sgd_optim, loss='sparse_categorical_crossentropy', metrics=[acc_metric,fixed_iou])\n",
    "# model.compile(optimizer=sgd_optim, loss=jaccard_distance, metrics=[acc_metric,fixed_iou])\n",
    "# model.compile(optimizer=sgd_optim, loss=categorical_focal_loss(gamma=2., alpha=.25), metrics=[acc_metric,fixed_iou])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"../checkpoint_dir/\" + experiment_name + datetime.now().strftime(\"-%Y%m%d-%Hh%Mmn\") + \"/best-cp-\" + experiment_name\n",
    "tb_log_dir      = \"../log_dir/\" + experiment_name + datetime.now().strftime(\"-%Y%m%d-%Hh%Mmn\")\n",
    "# callbacks\n",
    "checkpoint_cb  = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, monitor='val_mean_iou', mode='max', save_weights_only=True, save_best_only=True, verbose=1)\n",
    "earlystop_cb   = tf.keras.callbacks.EarlyStopping(monitor='val_mean_iou', mode='max', patience=10, verbose=1)\n",
    "reduce_cb      = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_Accuracy', min_delta=0.0001, factor=0.5, patience=5, mode='max', verbose=1)\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(log_dir=tb_log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classic Training Strategy:\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_history = model.fit(train_ds,\n",
    "                          steps_per_epoch=TRAIN_STEPS_PER_EPOCH,\n",
    "                          epochs=100,\n",
    "                          validation_data=val_ds,\n",
    "                          validation_steps=VAL_STEPS_PER_EPOCH,\n",
    "#                           callbacks=[checkpoint_cb, reduce_cb, earlystop_cb, tensorboard_cb],\n",
    "                          callbacks=[checkpoint_cb, reduce_cb, tensorboard_cb],\n",
    "#                           callbacks=[checkpoint_cb, reduce_cb],\n",
    "                         verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_keras_history(keras_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_keras_history(keras_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_keras_history(keras_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and loading the model:\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if one wants to save the model weights manually (keras checkpoint callback are preferred)\n",
    "model.save_weights('../saved_models/you_directory_name/model_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if one wants to load the waits from a specific directory (model should be built beforehand)\n",
    "model.load_weights('../saved_models/you_directory_name/model_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if one wants to save the full model (architecture + weights): NOT WORKING because of the custon Mean IOU metric\n",
    "model.save('../saved_models/saved_full_models/you_directory_name/model_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_new = get_unet_model(MODEL_SHAPE, filters=32, n_block=4, n_class=6, mode='serial', residual=True)\n",
    "model_new.compile(optimizer=sgd_optim, loss='sparse_categorical_crossentropy', metrics=[acc_metric,fixed_iou])\n",
    "model_new.load_weights('../saved_models/isprs_saved_weights/best-cp-unet_input256_noScale_skip4_Residual_serialCenter_sgd_jaccardloss_bs50_ocpCB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking that we obtain the same metric performance after loading the weights\n",
    "model_new.evaluate(val_ds, steps=VAL_STEPS_PER_EPOCH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics Computation:\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, load the best model reached during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance before loading the weights\n",
    "model.evaluate(val_ds, steps=VAL_STEPS_PER_EPOCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('../checkpoint_dir_part2/unet_input256_noScale_skip4_Residual_serialCenter_sgd_jaccardloss_bs50_ocpCB-20191129-10h43mn/best-cp-unet_input256_noScale_skip4_Residual_serialCenter_sgd_jaccardloss_bs50_ocpCB')\n",
    "# model.load_weights('../saved_models/isprs_saved_weights/best-cp-unet_input256_noScale_skip4_Residual_serialCenter_sgd_jaccardloss_bs50_ocpCB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance after loading the best weights\n",
    "model.evaluate(val_ds, steps=VAL_STEPS_PER_EPOCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can compute the IoU for a given patch pairs\n",
    "compute_batch_iou(model, patch_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or over the whole dataset: output the confusion matrix and per class IOU\n",
    "compute_dataset_iou(model, val_ds, VAL_STEPS_PER_EPOCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also output only the normalized confusion matrix (accuracy on diagonal)\n",
    "cm = compute_dataset_cm(model, val_ds, VAL_STEPS_PER_EPOCH, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cm(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output the confusion matrix with pretty printing\n",
    "pd.options.display.float_format = '{:3.2e}'.format\n",
    "data = cm.numpy().astype(float)\n",
    "df = pd.DataFrame(data, columns=color_names, index= color_names)\n",
    "df['Total'] = df.sum(axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate Finder:\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrf = LearningRateFinder(model)\n",
    "lrf.find(train_ds, 1e-6, 1e+1, epochs=1, stepsPerEpoch=TRAIN_STEPS_PER_EPOCH)\n",
    "lrf.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrf.find(train_ds, 1e-6, 1e+1, epochs=2, stepsPerEpoch=TRAIN_STEPS_PER_EPOCH)\n",
    "lrf.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrf.find(train_ds, 1e-6, 1, epochs=3, stepsPerEpoch=TRAIN_STEPS_PER_EPOCH)\n",
    "lrf.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resetModel(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Cycle Policy Training:\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocp_cb = OneCycleScheduler(max_lr=5e-2)\n",
    "checkpoint_path = \"../checkpoint_dir/\" + experiment_name + datetime.now().strftime(\"-%Y%m%d-%Hh%Mmn\") + \"/best-cp-\" + experiment_name\n",
    "tb_log_dir      = \"../log_dir/\" + experiment_name + datetime.now().strftime(\"-%Y%m%d-%Hh%Mmn\")\n",
    "\n",
    "checkpoint_cb  = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, monitor='val_mean_iou', mode='max', save_weights_only=True, save_best_only=True, verbose=1)\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(log_dir=tb_log_dir)\n",
    "\n",
    "keras_history = model.fit(train_ds, \n",
    "                   steps_per_epoch=TRAIN_STEPS_PER_EPOCH,\n",
    "                   epochs=100,\n",
    "                   validation_data=val_ds,\n",
    "                   validation_steps=VAL_STEPS_PER_EPOCH,\n",
    "                   callbacks=[ocp_cb, checkpoint_cb, tensorboard_cb],\n",
    "                   verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocp_cb.plot_lr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_keras_history(keras_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(val_ds, steps=VAL_STEPS_PER_EPOCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_weight = model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('../checkpoint_dir/skip4_resunet_300_jl_sgd_ocp-20191111-20h47mn/best-cp-skip4_resunet_300_jl_sgd_ocp')\n",
    "model.evaluate(val_ds, steps=VAL_STEPS_PER_EPOCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_dataset_iou(model, val_ds, VAL_STEPS_PER_EPOCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_dataset_iou(model, val_ds, VAL_STEPS_PER_EPOCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('../saved_models/saved_weight_models/base_256input_sgd_ocp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resetModel(model)\n",
    "model.evaluate(val_ds, steps=VAL_STEPS_PER_EPOCH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying custom metric for each class:\n",
    "---\n",
    "\n",
    "Instead of computing Mean Intersection over Union during training, we can also computer per class IoU (as demonstrated by this code). But this solution is not suitable for two reasons:\n",
    "\n",
    "- first we have 6 classes, it will clutter the log during keras training\n",
    "- second, for validation set, a proper implementation should consider a cumulative IoU over the whole dataset. This implementation is called after each batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "@tf.function\n",
    "def iou(y_true, y_pred, label: int):\n",
    "    \"\"\"\n",
    "    Return the Intersection over Union (IoU) for a given label.\n",
    "    Args:\n",
    "        y_true: the expected y values as a one-hot\n",
    "        y_pred: the predicted y values as a one-hot or softmax output\n",
    "        label: the label to return the IoU for\n",
    "    Returns:\n",
    "        the IoU for the given label\n",
    "    \"\"\"\n",
    "    # extract the label values using the argmax operator then\n",
    "    # calculate equality of the predictions and truths to the label\n",
    "    y_true = K.cast(K.equal(y_true, label), K.floatx())\n",
    "    y_pred = K.cast(K.equal(K.argmax(y_pred, axis=-1), label), K.floatx())\n",
    "    # calculate the |intersection| (AND) of the labels\n",
    "    intersection = K.sum(y_true * y_pred)\n",
    "    # calculate the |union| (OR) of the labels\n",
    "    union = K.sum(y_true) + K.sum(y_pred) - intersection\n",
    "    # avoid divide by zero - if the union is zero, return 1\n",
    "    # otherwise, return the intersection over union\n",
    "    return K.switch(K.equal(union, 0), 1.0, intersection / union)\n",
    "\n",
    "\n",
    "def build_iou_for(label: int, name: str=None):\n",
    "    \"\"\"\n",
    "    Build an Intersection over Union (IoU) metric for a label.\n",
    "    Args:\n",
    "        label: the label to build the IoU metric for\n",
    "        name: an optional name for debugging the built method\n",
    "    Returns:\n",
    "        a keras metric to evaluate IoU for the given label\n",
    "        \n",
    "    Note:\n",
    "        label and name support list inputs for multiple labels\n",
    "    \"\"\"\n",
    "    # handle recursive inputs (e.g. a list of labels and names)\n",
    "    if isinstance(label, list):\n",
    "        if isinstance(name, list):\n",
    "            return [build_iou_for(l, n) for (l, n) in zip(label, name)]\n",
    "        return [build_iou_for(l) for l in label]\n",
    "\n",
    "    # build the method for returning the IoU of the given label\n",
    "    def label_iou(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Return the Intersection over Union (IoU) score for {0}.\n",
    "        Args:\n",
    "            y_true: the expected y values as a one-hot\n",
    "            y_pred: the predicted y values as a one-hot or softmax output\n",
    "        Returns:\n",
    "            the scalar IoU value for the given label ({0})\n",
    "        \"\"\".format(label)\n",
    "        return iou(y_true, y_pred, label)\n",
    "\n",
    "    # if no name is provided, us the label\n",
    "    if name is None:\n",
    "        name = label\n",
    "    # change the name of the method for debugging\n",
    "    label_iou.__name__ = 'iou_{}'.format(name)\n",
    "\n",
    "    return label_iou\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_iou = build_iou_for(label=[0,1,2,3,4,5], name=color_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weight.compile(optimizer=adam_optim, loss='sparse_categorical_crossentropy', metrics=[acc_metric,fixed_iou, *class_iou])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = model_weight.evaluate(val_ds, steps=VAL_STEPS_PER_EPOCH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Futur test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"An implementation of the Intersection over Union (IoU) metric for Keras.\"\"\"\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "def iou(y_true, y_pred, label: int):\n",
    "    \"\"\"\n",
    "    Return the Intersection over Union (IoU) for a given label.\n",
    "    Args:\n",
    "        y_true: the expected y values as a one-hot\n",
    "        y_pred: the predicted y values as a one-hot or softmax output\n",
    "        label: the label to return the IoU for\n",
    "    Returns:\n",
    "        the IoU for the given label\n",
    "    \"\"\"\n",
    "    # extract the label values using the argmax operator then\n",
    "    # calculate equality of the predictions and truths to the label\n",
    "    y_true = K.cast(K.equal(K.argmax(y_true), label), K.floatx())\n",
    "    y_pred = K.cast(K.equal(K.argmax(y_pred), label), K.floatx())\n",
    "    # calculate the |intersection| (AND) of the labels\n",
    "    intersection = K.sum(y_true * y_pred)\n",
    "    # calculate the |union| (OR) of the labels\n",
    "    union = K.sum(y_true) + K.sum(y_pred) - intersection\n",
    "    # avoid divide by zero - if the union is zero, return 1\n",
    "    # otherwise, return the intersection over union\n",
    "    return K.switch(K.equal(union, 0), 1.0, intersection / union)\n",
    "\n",
    "\n",
    "def build_iou_for(label: int, name: str=None):\n",
    "    \"\"\"\n",
    "    Build an Intersection over Union (IoU) metric for a label.\n",
    "    Args:\n",
    "        label: the label to build the IoU metric for\n",
    "        name: an optional name for debugging the built method\n",
    "    Returns:\n",
    "        a keras metric to evaluate IoU for the given label\n",
    "        \n",
    "    Note:\n",
    "        label and name support list inputs for multiple labels\n",
    "    \"\"\"\n",
    "    # handle recursive inputs (e.g. a list of labels and names)\n",
    "    if isinstance(label, list):\n",
    "        if isinstance(name, list):\n",
    "            return [build_iou_for(l, n) for (l, n) in zip(label, name)]\n",
    "        return [build_iou_for(l) for l in label]\n",
    "\n",
    "    # build the method for returning the IoU of the given label\n",
    "    def label_iou(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Return the Intersection over Union (IoU) score for {0}.\n",
    "        Args:\n",
    "            y_true: the expected y values as a one-hot\n",
    "            y_pred: the predicted y values as a one-hot or softmax output\n",
    "        Returns:\n",
    "            the scalar IoU value for the given label ({0})\n",
    "        \"\"\".format(label)\n",
    "        return iou(y_true, y_pred, label)\n",
    "\n",
    "    # if no name is provided, us the label\n",
    "    if name is None:\n",
    "        name = label\n",
    "    # change the name of the method for debugging\n",
    "    label_iou.__name__ = 'iou_{}'.format(name)\n",
    "\n",
    "    return label_iou\n",
    "        \n",
    "\n",
    "def mean_iou(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Return the Intersection over Union (IoU) score.\n",
    "    Args:\n",
    "        y_true: the expected y values as a one-hot\n",
    "        y_pred: the predicted y values as a one-hot or softmax output\n",
    "    Returns:\n",
    "        the scalar IoU value (mean over all labels)\n",
    "    \"\"\"\n",
    "    # get number of labels to calculate IoU for\n",
    "    num_labels = K.int_shape(y_pred)[-1]\n",
    "    # initialize a variable to store total IoU in\n",
    "    total_iou = K.variable(0)\n",
    "    # iterate over labels to calculate IoU for\n",
    "    for label in range(num_labels):\n",
    "        total_iou = total_iou + iou(y_true, y_pred, label)\n",
    "    # divide total IoU by number of labels to get mean IoU\n",
    "    return total_iou / num_labels\n",
    "\n",
    "\n",
    "# explicitly define the outward facing API of this module\n",
    "__all__ = [build_iou_for.__name__, mean_iou.__name__]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (POC) Image Reconstruction from patches w/o looping:\n",
    "---\n",
    "In this section we study how to predict over an entire tile: we extract patches, predict over them and then reconstruct the prediction mask using the `tf.scatter_nd` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = imageio.imread(val_images[2])\n",
    "img = tf.image.convert_image_dtype(img,tf.float32)\n",
    "msk = imageio.imread(val_masks[2])\n",
    "msk = tf.image.convert_image_dtype(msk,tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_show_pair((img, msk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_tensor = tf.ones_like(img)\n",
    "# patches = tf.image.extract_patches(images=tf.expand_dims(img, axis=0), sizes=SIZES, strides=STRIDES, rates=RATES, padding=PADDING)\n",
    "patches = tf.image.extract_patches(images=[img,one_tensor], sizes=SIZES, strides=STRIDES, rates=[1, 3, 3, 1], padding=PADDING)\n",
    "nrow, ncol = patches.shape[1:3]\n",
    "patches = tf.reshape(patches, [2, nrow*ncol,PATCH_SIZE,PATCH_SIZE,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_show_pair((patches[0,0,:], patches[1,0,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the index patching trick to reconstruct image from the extracted patches\n",
    "one_tensor = tf.ones_like(img)\n",
    "# patches = tf.image.extract_patches(images=tf.expand_dims(img, axis=0), sizes=SIZES, strides=STRIDES, rates=RATES, padding=PADDING)\n",
    "patches = tf.image.extract_patches(images=[img,one_tensor], sizes=SIZES, strides=STRIDES, rates=RATES, padding=PADDING)\n",
    "z = patches[0]\n",
    "z = tf.squeeze(z) # output shape=(17, 17, 786432)\n",
    "z = tf.reshape(z, [17*17,PATCH_SIZE,PATCH_SIZE,3]) # TensorShape([289, 512, 512, 3])\n",
    "one_patches = patches[1]\n",
    "one_patches = tf.squeeze(one_patches) # output shape=(17, 17, 786432)\n",
    "one_patches = tf.reshape(one_patches, [17*17,PATCH_SIZE,PATCH_SIZE,3]) # TensorShape([289, 512, 512, 3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patches[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.range(6000)\n",
    "y = tf.range(6000)\n",
    "x, y = tf.meshgrid(x, y)\n",
    "indices = tf.stack([y, x], axis=-1)\n",
    "indices_patches = tf.image.extract_patches(images=tf.expand_dims(indices, axis=0), sizes=SIZES, strides=STRIDES, rates=RATES, padding=PADDING) \n",
    "indices_patches =  tf.squeeze(indices_patches) # shape=(17, 17, 524288)\n",
    "indices_patches = tf.reshape(indices_patches, [17*17,PATCH_SIZE,PATCH_SIZE,2]) # TensorShape([289, 512, 512, 2])\n",
    "indices_patches\n",
    "reconstructed = tf.scatter_nd(indices=indices_patches, updates=z, shape=(6000,6000,3))\n",
    "overlap =  tf.scatter_nd(indices=indices_patches, updates=one_patches, shape=(6000,6000,3))\n",
    "final = tf.math.truediv(reconstructed, overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reduce_all(final == img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(z, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_mask = tf.scatter_nd(indices=indices_patches, updates=preds, shape=(6000,6000,6))\n",
    "reconstructed_mask = tf.argmax(reconstructed_mask, axis=-1, output_type=tf.int32)\n",
    "reconstructed_mask = label2mask(reconstructed_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,20))\n",
    "plt.imshow(reconstructed_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting on new Tiles:\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_path = '/datasets/obliquetest/cal004image0000152.jpg'\n",
    "# img_path = 'RI-39-GEOTIFF-195-20150316174501569000c47.tif'\n",
    "idx = 2\n",
    "img_path = val_images[idx]\n",
    "msk_path = val_masks[idx]\n",
    "img = imageio.imread(img_path)\n",
    "img = tf.image.convert_image_dtype(img,tf.float32)\n",
    "msk = imageio.imread(msk_path)\n",
    "msk = tf.image.convert_image_dtype(msk,tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.subplots(figsize=(20,20))\n",
    "# plt.imshow(img)\n",
    "my_show_pair((img,msk),interpolation='gaussian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = get_tile_prediction(img_path, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_show_pair((img,y_pred),interpolation='gaussian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_show_pair((msk,y_pred),interpolation='gaussian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_tile_iou(msk, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = get_tile_cm(msk, y_pred, normalize=True)\n",
    "plot_cm(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Time Augmentation in order to cover the right and bottom edges of the tile (red bands)\n",
    "y_pred2 = get_tile_tta_pred(img_path, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_show_pair((msk,y_pred2), interpolation='gaussian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_tile_iou(msk, y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm2 = get_tile_cm(msk, y_pred2, normalize=True)\n",
    "plot_cm(cm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can even overlap the aerial image with the prediction mask (too much clutter)\n",
    "plt.subplots(figsize=(20,20))\n",
    "plt.imshow(img)\n",
    "plt.imshow(y_pred2, alpha=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 3\n",
    "img_path = val_images[idx]\n",
    "msk_path = val_masks[idx]\n",
    "img = imageio.imread(img_path)\n",
    "img = tf.image.convert_image_dtype(img,tf.float32)\n",
    "msk = imageio.imread(msk_path)\n",
    "msk = tf.image.convert_image_dtype(msk,tf.float32)\n",
    "y_pred2 = get_tile_tta_pred(img_path, model)\n",
    "my_show_pair((msk,y_pred2), interpolation='gaussian')\n",
    "get_tile_iou(msk, y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 4\n",
    "img_path = val_images[idx]\n",
    "msk_path = val_masks[idx]\n",
    "img = imageio.imread(img_path)\n",
    "img = tf.image.convert_image_dtype(img,tf.float32)\n",
    "msk = imageio.imread(msk_path)\n",
    "msk = tf.image.convert_image_dtype(msk,tf.float32)\n",
    "y_pred2 = get_tile_tta_pred(img_path, model)\n",
    "my_show_pair((msk,y_pred2), interpolation='gaussian')\n",
    "get_tile_iou(msk, y_pred2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INRIA Aerial Dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining tiff image and mask location\n",
    "inria_train_img_dir  = '/datasets/InriaAerial/AerialImageDataset/train/images/'\n",
    "inria_train_mask_dir = '/datasets/InriaAerial/AerialImageDataset/train/gt/'\n",
    "inria_val_img_dir    = '/datasets/InriaAerial/AerialImageDataset/valid/images/'\n",
    "inria_val_mask_dir   = '/datasets/InriaAerial/AerialImageDataset/valid/gt/'\n",
    "# let's build two list containing the path of the aerials and mask files\n",
    "train_src = Path(inria_train_img_dir)\n",
    "train_ref = Path(inria_train_mask_dir)\n",
    "val_src   = Path(inria_val_img_dir)\n",
    "val_ref   = Path(inria_val_mask_dir)\n",
    "train_images     = sorted([str(x) for x in train_src.iterdir() if x.is_file() and x.suffix == '.tif'])\n",
    "train_masks      = sorted([str(x) for x in train_ref.iterdir() if x.is_file() and x.suffix == '.tif'])\n",
    "val_images       = sorted([str(x) for x in val_src.iterdir()   if x.is_file() and x.suffix == '.tif'])\n",
    "val_masks        = sorted([str(x) for x in val_ref.iterdir()   if x.is_file() and x.suffix == '.tif'])\n",
    "\n",
    "# checking the equality of the two list lenghts\n",
    "assert len(train_images) == len(train_masks), \" (Error) The train Aerial image count does not match Mask counts!\"\n",
    "assert len(val_images)   == len(val_masks), \" (Error) The Validation Aerial image count does not match Mask counts!\"\n",
    "len_train_images = len(train_images)\n",
    "len_val_images   = len(val_images)\n",
    "len_train_images, len_val_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ordered lists of colors and it's corresponding class name in [R, G, B] format\n",
    "color_list  = [ [0, 0, 0], [1, 1, 1]]\n",
    "color_names = ['Background', 'Building']\n",
    "# patch extraction parameters\n",
    "TILE_SIZE   = 5000 \n",
    "PATCH_SIZE  = 256\n",
    "PATCH_STRIDE= 256\n",
    "PATCH_RATE  = 1\n",
    "SIZES       = [1, PATCH_SIZE, PATCH_SIZE, 1] \n",
    "STRIDES     = [1, PATCH_STRIDE, PATCH_STRIDE, 1] \n",
    "RATES       = [1, PATCH_RATE, PATCH_RATE, 1] \n",
    "PADDING     = 'VALID'\n",
    "\n",
    "def compute_tile_patch_number(rates=[1,2,3]):\n",
    "    \"\"\" we expect that all tile share the same size \"\"\"\n",
    "    with tf.device('/CPU:0'):\n",
    "        aerial = tf.constant(imageio.imread(train_images[0]))\n",
    "        total  = 0\n",
    "        for r in rates:\n",
    "            patches = tf.image.extract_patches(images=tf.expand_dims(aerial, axis=0), sizes=SIZES, strides=STRIDES, rates=[1,r,r,1], padding=PADDING)            \n",
    "            total += patches.shape[1] * patches.shape[2]\n",
    "        return total\n",
    "\n",
    "TRAIN_PATCH_NUMBER = compute_tile_patch_number(rates=[1])\n",
    "VALID_PATCH_NUMBER = compute_tile_patch_number(rates=[1])\n",
    "TRAIN_PATCH_NUMBER, VALID_PATCH_NUMBER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SHAPE  = (256, 256, 3)\n",
    "PATCH_RESIZE = (128, 128)\n",
    "NUM_CLASSES  = 2\n",
    "BATCH_SIZE   = 50\n",
    "EPOCHS       = 50\n",
    "NUM_TRAIN_EXAMPLES = TRAIN_PATCH_NUMBER * len_train_images \n",
    "NUM_VAL_EXAMPLES   = VALID_PATCH_NUMBER * len_val_images \n",
    "LEARNING_RATE   = 0.0001\n",
    "TRAIN_STEPS_PER_EPOCH = int(np.ceil(NUM_TRAIN_EXAMPLES / float(BATCH_SIZE)))\n",
    "VAL_STEPS_PER_EPOCH   = int(np.ceil(NUM_VAL_EXAMPLES / float(BATCH_SIZE)))\n",
    "TRAIN_STEPS_PER_EPOCH, VAL_STEPS_PER_EPOCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = get_dataset(img_dir=inria_train_img_dir, mask_dir=inria_train_mask_dir, batch_size=BATCH_SIZE, shuffle=True, repeat=True, augment=True, cachefile='inria_train_cache')\n",
    "val_ds   = get_dataset(img_dir=inria_val_img_dir, mask_dir=inria_val_mask_dir,batch_size=BATCH_SIZE, shuffle=False, repeat=True, augment=False, cachefile=None)\n",
    "print(\"train patch number = {}, valid patch number = {}\".format(TRAIN_PATCH_NUMBER, VALID_PATCH_NUMBER))\n",
    "print(\"train steps = {}, valid steps = {}\".format(TRAIN_STEPS_PER_EPOCH, VAL_STEPS_PER_EPOCH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timepipeline(train_ds, batch_size=BATCH_SIZE, steps=4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timepipeline(val_ds, batch_size=BATCH_SIZE, steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator = iter(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting next pair of aerial image and its associated mask\n",
    "patch_pairs = next(train_iterator)\n",
    "# display images\n",
    "my_show_grid(patch_pairs, nrow=4, ncol=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inria_model = get_unet_model(MODEL_SHAPE, filters=32, n_block=4, n_class=2, mode=None, residual=True)\n",
    "reset_weights = inria_model.get_weights()\n",
    "resetModel = lambda m: m.set_weights(reset_weights)\n",
    "# usage: resetModel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inria_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"inria_unet_input256_noScale_skip4_Residual_residualCenter_sgd_cross_entropy_bs50_ocpCB\"\n",
    "sgd_optim = tf.keras.optimizers.SGD(lr=0.1, nesterov=True)\n",
    "adam_optim = tf.keras.optimizers.Adam(lr=1e-3)\n",
    "acc_metric = tf.keras.metrics.SparseCategoricalAccuracy(name='Accuracy')\n",
    "iou_metric = tf.keras.metrics.MeanIoU(num_classes=NUM_CLASSES)\n",
    "fixed_iou  = CategoricalMeanIoU(num_classes=NUM_CLASSES)\n",
    "inria_model.compile(optimizer=sgd_optim, loss='sparse_categorical_crossentropy', metrics=[acc_metric,fixed_iou])\n",
    "# inria_model.compile(optimizer=sgd_optim, loss=jaccard_distance, metrics=[acc_metric,fixed_iou])\n",
    "# inria_model.compile(optimizer=sgd_optim, loss=categorical_focal_loss(gamma=2., alpha=.25), metrics=[acc_metric,fixed_iou])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrf = LearningRateFinder(inria_model)\n",
    "lrf.find(train_ds, 1e-7, 1, epochs=1, stepsPerEpoch=TRAIN_STEPS_PER_EPOCH)\n",
    "lrf.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrf = LearningRateFinder(inria_model)\n",
    "lrf.find(train_ds, 1e-6, 1e+1, epochs=2, stepsPerEpoch=TRAIN_STEPS_PER_EPOCH)\n",
    "lrf.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classical training\n",
    "checkpoint_path = \"../inria_checkpoint_dir/\" + experiment_name + datetime.now().strftime(\"-%Y%m%d-%Hh%Mmn\") + \"/best-cp-\" + experiment_name\n",
    "tb_log_dir      = \"../inria_log_dir/\" + experiment_name + datetime.now().strftime(\"-%Y%m%d-%Hh%Mmn\")\n",
    "# callbacks\n",
    "checkpoint_cb  = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, monitor='val_mean_iou', mode='max', save_weights_only=True, save_best_only=True, verbose=1)\n",
    "earlystop_cb   = tf.keras.callbacks.EarlyStopping(monitor='val_mean_iou', mode='max', patience=10, verbose=1)\n",
    "reduce_cb      = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_Accuracy', min_delta=0.0001, factor=0.5, patience=5, mode='max', verbose=1)\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(log_dir=tb_log_dir)\n",
    "\n",
    "keras_history = inria_model.fit(train_ds,\n",
    "                          steps_per_epoch=TRAIN_STEPS_PER_EPOCH,\n",
    "                          epochs=100,\n",
    "                          validation_data=val_ds,\n",
    "                          validation_steps=VAL_STEPS_PER_EPOCH,\n",
    "#                           callbacks=[checkpoint_cb, reduce_cb, earlystop_cb, tensorboard_cb],\n",
    "                          callbacks=[checkpoint_cb, reduce_cb, tensorboard_cb],\n",
    "#                           callbacks=[checkpoint_cb, reduce_cb],\n",
    "                         verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One cycle policy training\n",
    "checkpoint_path = \"../inria_checkpoint_dir/\" + experiment_name + datetime.now().strftime(\"-%Y%m%d-%Hh%Mmn\") + \"/best-cp-\" + experiment_name\n",
    "tb_log_dir      = \"../inria_log_dir/\" + experiment_name + datetime.now().strftime(\"-%Y%m%d-%Hh%Mmn\")\n",
    "# callbacks\n",
    "ocp_cb = OneCycleScheduler(max_lr=1e-1)\n",
    "checkpoint_cb  = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, monitor='val_mean_iou', mode='max', save_weights_only=True, save_best_only=True, verbose=1)\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(log_dir=tb_log_dir)\n",
    "\n",
    "keras_history = inria_model.fit(train_ds, \n",
    "                   steps_per_epoch=TRAIN_STEPS_PER_EPOCH,\n",
    "                   epochs=70,\n",
    "                   validation_data=val_ds,\n",
    "                   validation_steps=VAL_STEPS_PER_EPOCH,\n",
    "                   callbacks=[ocp_cb, checkpoint_cb, tensorboard_cb],\n",
    "                   verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation:\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inria_model.evaluate(val_ds, steps=VAL_STEPS_PER_EPOCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inria_model.load_weights('../inria_checkpoint_dir/inria_unet_input256_noScale_skip4_Residual_residualCenter_sgd_cross_entropy_bs50_ocpCB-20191204-21h36mn/best-cp-inria_unet_input256_noScale_skip4_Residual_residualCenter_sgd_cross_entropy_bs50_ocpCB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inria_model.evaluate(val_ds, steps=VAL_STEPS_PER_EPOCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_dataset_iou(inria_model, val_ds, VAL_STEPS_PER_EPOCH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting on new Tiles:\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_path = '/datasets/obliquetest/cal004image0000152.jpg'\n",
    "# img_path = 'RI-39-GEOTIFF-195-20150316174501569000c47.tif'\n",
    "index = 7\n",
    "img_path = val_images[index]\n",
    "msk_path = val_masks[index]\n",
    "img = imageio.imread(img_path)\n",
    "# img = tf.image.convert_image_dtype(img,tf.float32)\n",
    "msk = imageio.imread(msk_path)\n",
    "# msk = tf.image.convert_image_dtype(msk,tf.float32)\n",
    "msk = msk // 255\n",
    "msk = tf.cast(msk, tf.int32)\n",
    "msk = tf.gather([[0, 0, 0], [255, 255, 255]], msk)\n",
    "msk = tf.cast(msk, tf.uint8)\n",
    "msk = tf.image.convert_image_dtype(msk,tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(20,20))\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.subplots(figsize=(20,20))\n",
    "# plt.imshow(img)\n",
    "my_show_pair((img,msk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = get_tile_prediction(img_path, inria_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_show_pair((msk,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_tile_iou(msk, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = get_tile_cm(msk, y_pred, normalize=True)\n",
    "plot_cm(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred2 = get_tile_tta_pred(img_path, inria_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_show_pair((msk,y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_tile_iou(msk, y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm2 = get_tile_cm(msk, y_pred2, normalize=True)\n",
    "plot_cm(cm2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marseille Tile:\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile = imageio.imread('/datasets/msebai_projects/marseille_15cm.tif')\n",
    "tile.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(20,20))\n",
    "plt.imshow(tile, interpolation='gaussian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = tile[20000:25000, 15000:20000,:]\n",
    "plt.subplots(figsize=(20,20))\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_tile = get_tile_tta_pred(img, model, from_disk=False)\n",
    "plt.subplots(figsize=(20,20))\n",
    "plt.imshow(y_pred_tile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_show_pair((img,y_pred_tile),interpolation='gaussian')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toulouse Tile:\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile = imageio.imread('~/datasets_link/MARSEILLE/cal000image0000185.jpg')\n",
    "tile.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(20,20))\n",
    "plt.imshow(tile, interpolation='gaussian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = tile[10000:16000, 10000:16000,:]\n",
    "img = tile\n",
    "plt.subplots(figsize=(20,20))\n",
    "plt.imshow(img,interpolation='gaussian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_tile = get_tile_tta_pred(img, model, from_disk=False)\n",
    "plt.subplots(figsize=(20,20))\n",
    "plt.imshow(y_pred_tile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_show_pair((img,y_pred_tile),interpolation='gaussian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_tile = get_tile_tta_pred(img, inria_model, from_disk=False)\n",
    "my_show_pair((img,y_pred_tile),interpolation='gaussian')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
